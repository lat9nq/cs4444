\documentclass[12pt]{article}
\title{Assignment 1: Sequential Program Optimization}
\author{Louis Thomas}
\date{30 January 2018}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{setspace}
\doublespacing
\usepackage{amsmath}
\begin{document}
	\maketitle
	\section{Problem Description}
	The goal of this assignment is to take some intentionally slow code and optimize it.
	The given code (written by Ross Walker) was purposefully written to let students take the code 
	and, between using compiler optimizations and hand-written ones, 
	make the resulting executable process the same data at a much faster pace 
	while producing the same or "same-enough" result.

	The computations performed by the computer were modeled after a fictional function: 
	\[ \displaystyle\sum_{j<i}\left(
			\frac{e^{(r_{i,j}*q_i)}*e^{(r_{i,j}*q_j)}}{r_{i,j}}
			-\frac{1}{a}
			\right) \text{ if } r_{i,j} \leq cut\]
	where $i$ and $j$ are some particle,
	$r_{i,j}$ is the distance from particles $i$ and $j$,
	$cut$ is the cut-off distance between two particles,
	$q_{i}$ is the energy scalar of particle $i$, and
	$a$ is a constant.

	The specific portion of the program that can benefit from optimization is a doubly-nested for-loop. Notable functions in the loop include $exp()$ and $sqrt()$.

	\section{Approach}

	The first optimization was to reduce a number of subtractions done within array indices on each loop iteration.
	The original loop counted from one to the number of atoms,
	however, $1$ was subtracted from $i$ and $j$ each time they were used inside the loop.
	The change included starting $i$ and $j$ from zero and changing the $\leq$ to a $<$, and removing all the $-1$'s from the array indices.

	Secondly, an if statement including a check for $i<j$ was removed and instead incorporated into the inner loop.
	This cuts about half of the iterations from only checking that $(j<i)$ is false and continuing.

	Third, a call to the square root function was moved until later.
	The original loop calculated $r_{i,j}$ in its entirety each iteration.
	This meant that, even if the value isn't included in the final sum, it was still calling the square root on it.
	Instead of comparing $r_{i,j}$ to the cutoff, $r_{i,j}^2$ is compared to $cut^2$.
	This required the small cost of calculating $cut^2$,
	but it also meant that $sqrt(r_{i,j}^2)$ was pushed back until it was necessary to be calcualted.

	Fourth, many dereferences were moved out of the inner-most loop.
	This causes some dereferences to only happen $1$ or $n$ times, as opposed to up to $\frac{1}{2}n^2$ times.
	The arrays that were acted on included the $coords$ and $q$ arrays.

	Fifth, the multiplying two exponents of the same base can be simplified to one exponentiation.
	From the original formula: $e^{r_{i,j}*q_i}*e^{r_{i,j}*q_j}$ can be simplified to $e^{r_{i,j}*q_i+r_{i,j}*q_j}$, and then $e^{r_{i,j}*(q_i+q_j)}$.
	An optimization in the code looks much like the previous simplification, 
	thus not only reducing the number of function calls but also cutting the number of exponentiations by half.

	A simple optimization follows from a mathematical characteristic in sums. 
	Specifically, $\displaystyle\sum_{i=1}^n{i} = n*i$.
	Thus, we can avoid subtracting $^1/_a$ each iteration, and instead subtract $x*{^1/_a}$ from the sum,
	where $x$ is the number of results within the cutoff.

	Finally, a optimization involved taking the two-dimensional \texttt{coords} array and converting it into a one-dimensional array of structs. 
	The hope here was to imporve cache hits and reduce misses.

	These tests were run on node granger3.

	%\section{Specifications}

	%The computer used is a Dell Optiplex 960 SFF running with 4x2 GiB of 800MHz DDR2 RAM and an Intel Core 2 Quad Q9650 (4x3.00 GHz).
	%The operating system is Xubuntu 17.10 (GNU/Linux 4.13.0-32-generic).
	%The compiler is gcc 7.2.0-8ubuntu3.
	%The hard drive is a Western Digital 250 GB SSD.

	%The program was tested on 10,000 data points generated from generate\_input.c on seed 10.

	\section{Observations}

	Some optimizations had nearly no effect on processing time, while others substantially contributed to the program's efficiency.

	\singlespacing
	\begin{center}
	\begin{tabular}{| p{3cm} | p{4cm} | p{1.5cm} | p{1.5cm} | l |}
	\hline
	Compiler flags & Optimization & Time to calculate $E$ (s) & Total Time (s) & Result \\ \hline
	None & & 2.3691 & 2.3775 & 58261662.7646147534 \\ \hline
	-O1 & & 1.0806 & 1.0893 & 58261662.7646147534 \\ \hline
	-O2 & & 0.9236 & 0.9318 & 58261662.7646147534 \\ \hline
	-O3 & & 0.9155 & 0.9232 & 58261662.7646147534 \\ \hline
	-Ofast & & 0.7045 & 0.7130 & 58261662.7646147460 \\ \hline
	-O3 & For-loops start from 0 & 0.9620 & 0.9695 & 58261662.7646147534 \\ \hline
	-O3 & Re-aligned for-loops & 0.8882 & 0.8961 & 58261662.7646147534 \\ \hline
	-O3 & Bring out dereferences & 0.8993 & 0.9082 & 58261662.7646147534 \\ \hline
	-O3 & Written-out squaring & 0.9291 & 0.9380 & 58261662.7646147534 \\ \hline
	-O3 & Combining exponents & 0.6981 & 0.7071 & 58261662.7646147460 \\ \hline
	-O3 & Comparing $r_{i,j}^2$ to $cut^2$ & 0.8461 & 0.8547 & 58261654.5481551588 \\ \hline
	-O3 & Subtract $x*{^1/_a}$ later & 1.1425 & 1.1512 & 58261662.7646149173 \\ \hline
	-O3 & Convert 2D array to 1D struct array & 0.8772 & 0.8853 & 58261662.7646147534 \\ \hline
	-O3 & All above & 0.5425 & 0.5521 & 58261654.5481553227 \\ \hline
	-O3 -ffast-math & All above & 0.5132 & 0.5223 & 58261654.5481553227 \\ \hline
	-Ofast & All above & 0.5104 & 0.5188 & 58261654.5481553227 \\ \hline
	-Ofast -fno-stack-protector & All above & 0.4971 & 0.5047 & 58261654.5481553227 \\ \hline
	\end{tabular}
	\end{center}
	\doublespacing

	\section{Analysis}

	The first notable observation is the magnitude of time difference just the first -O1 flag provides. 
	Also notable is the insignificant difference between -O3 and -O2 compilation flags on the original program.
	-O3 has at best no effect, likely a slight adverse effect, compared to -O2. 

	Next, -Ofast on the original program produces not only a faster program, but also a slightly different result.
	The documentation indicates that -Ofast produces code that acts in a different manner from the IEEE specification, and that fact is apparent here.

	Some optimizations resulted in a increase of time taken.
	This may be because those optimizations work better with other optimizations, 
	such as in writing out the squaring as opposed to using the \texttt{pow()} method.
	There is also a difference in the produced result on the change to checking $r_{i,j}^2$ against $cut^2$.
	The best explanation is the gained resolution from not using the \texttt{sqrt()} function.
	Since \texttt{sqrt()} is normally implemented through an approximation requiring many iterations of floating-point operations,
	resolution is likely lost through the \texttt{sqrt()}.

	Removing one of the buffer overflow defenses, checking for stack smashing, seems to produce a small time advantage.
	In a practical implementation, this may or may not be a preferrable optimization 
	since it cuts out two-thirds of the protections against buffer overflow strikes, but it is interesting to see that it does provide a small time reduction.
	This optimization may prove to be more effective if there were more user-written functions called to during the execution of the program.

	\section{Conclusions}

	-O1 provides the majority of the compiler-level optimizations in this situation, and marginal improvements to execution time are added by -O2 or -O3.
	It is possible that some of the adverse affects of -O3 versus -O2 are due to general optimizations that would, on average, be effective optimizations.
	-Ofast enables -ffast-math and, thus, many unsafe but faster optimizations to the program.
	In addition, combining -O2 and -ffast-math is not faster than -Ofast, despite -Ofast is defined as -O3 -ffast-math when -O2 is when than -O3.
	This is perhaps because -O3 has optimizations that work better for -ffast-math than -O2.

	In general, making the program do less computations results in less execution time, but can result in varrying mathematical results.
	Varrying optimizations can have varrying magnitudes of change in the calculated result.

	Effective optimizations the programmer can take include reducing calls to heavy math functions, 
		reducing dereferences to arrays, 
		and putting off calculations until later when it is mathematically sound to do so and results in less computations.

	\section{Pledge}
	On my honor, I pledge that I did not give nor receive help on this assignment.

	\vspace{1cm}

	Louis Thomas

\end{document}
